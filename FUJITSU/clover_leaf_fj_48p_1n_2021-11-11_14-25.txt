Command:        /home/srivad01/REPOS/CloverLeaf3D_ref/exe_builds/A64FX_runs/clover_leaf_fj
Resources:      1 node (48 physical, 48 logical cores per node)
Memory:         32 GiB per node
Tasks:          48 processes, OMP_NUM_THREADS was 0
Machine:        pacu12
Start time:     Thu Nov 11 14:25:17 2021
Total time:     17 seconds
Full path:      /home/srivad01/REPOS/CloverLeaf3D_ref/exe_builds/A64FX_runs

Summary: clover_leaf_fj is Compute-bound in this configuration
Compute:                                     66.1% |======|
MPI:                                         33.9% |==|
I/O:                                      &lt;0.1% ||
This application run was Compute-bound. A breakdown of this time and advice for investigating further is in the CPU Metrics section below. 

CPU Metrics:
Linux perf event metrics:
Single-core code:                             1.0% ||
OpenMP regions:                              99.0% |=========|
Cycles per instruction:                       1.97 
L2D cache miss:                              34.7% |==|
Stalled backend cycles:                      66.8% |======|
Stalled frontend cycles:                      7.7% ||
A high number of cycles are stalled in the CPU. A high amount of memory accesses could be responsible for the non-exploitation of all the CPU cycles.

MPI:
A breakdown of the 33.9% MPI time:
Time in collective calls:                    18.0% |=|
Time in point-to-point calls:                82.0% |=======|
Effective process collective rate:            3.16 kB/s
Effective process point-to-point rate:         339 MB/s
Most of the time is spent in point-to-point calls with an average transfer rate. Using larger messages and overlapping communication and computation may increase the effective transfer rate.

I/O:
A breakdown of the <0.1% I/O time:
Time in reads:                               25.0% |==|
Time in writes:                              75.0% |=======|
Effective process read rate:                  38.8 GB/s
Effective process write rate:                 1.08 MB/s
Most of the time is spent in write operations with a very low effective transfer rate. This may be caused by contention for the filesystem or inefficient access patterns. Use an I/O profiler to investigate which write calls are affected.

OpenMP:
A breakdown of the 99.0% time in OpenMP regions:
Computation:                                  0.6% ||
Synchronization:                             99.4% |=========|
Physical core utilization:                  100.0% |=========|
System load:                                100.2% |=========|
Significant time is spent synchronizing threads in parallel regions. Check the affected regions with a profiler.
This may be a sign of overly fine-grained parallelism (OpenMP regions in tight loops) or workload imbalance.

Memory:
Per-process memory usage may also affect scaling:
Mean process memory usage:                    56.4 MiB
Peak process memory usage:                    62.6 MiB
Peak node memory usage:                      46.0% |====|
The peak node memory usage is low. Running with fewer MPI processes and more data on each process may be more efficient.

Energy:
A breakdown of how energy was used:
CPU:                                      not supported
System:                                   not supported
Mean node power:                          not supported
Peak node power:                              0.00 W
Energy metrics are not available on this system.

